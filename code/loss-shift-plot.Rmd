---
title: "loss_shift_plot"
author: "Taenyoung Lee"
date: "`r format(Sys.time(), '%Y년 %B %d일')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
    code_folding: hide
---

## test function

```{r, results='hide'}
# ========================================================================
# 0) 라이브러리 -----------------------------------------------------------
suppressPackageStartupMessages({
  library(e1071);  library(glmnet)
  library(dplyr);  library(tibble);  library(ggplot2);  library(tidyr)
})

# 1) Loss-Shift 함수 로드 --------------------------------------------------
# "loss_shifting.r" 파일이 작업 디렉토리에 있어야 합니다.
if (file.exists("loss_shifting.r")) {
    source("loss_shifting.r")  # train_loss_shift(), predict_loss_shift(), ...
} else {
    message("WARNING: 'loss_shifting.r' not found. Loss-Shift models will fail.")
}


# 2) 유틸리티 --------------------------------------------------------------
## (a) 지역(outlier) 생성 --------------------------------------------------
add_regional_outliers <- function(data, clusters, outlier_frac, sd = 0.05) {
  dfs <- list(data)
  for (cl in clusters) {
    n_extra <- if (outlier_frac >= 1) as.integer(outlier_frac) else
               ceiling(sum(data$y == cl$label, na.rm = TRUE) * outlier_frac)
    if (n_extra > 0) {
      pts <- matrix(rnorm(2 * n_extra, 0, sd), ncol = 2)
      dfs[[length(dfs) + 1]] <-
        data.frame(X1 = pts[, 1] + cl$cx,
                   X2 = pts[, 2] + cl$cy,
                   y  = cl$label)
    }
  }
  bind_rows(dfs)
}

## (b) 라벨 노이즈 추가 (수정됨) ---------------------------------------------
#' @title 라벨 노이즈 추가
#' @description 데이터의 클래스별로 지정된 비율만큼 라벨을 무작위로 뒤집습니다.
#' @param data 원본 데이터프레임. 'y' 컬럼을 포함해야 합니다.
#' @param flip_frac_pos Positive(+1) 클래스에서 라벨을 뒤집을 비율 (0 ~ 1).
#' @param flip_frac_neg Negative(-1) 클래스에서 라벨을 뒤집을 비율 (0 ~ 1).
#' @return 라벨이 뒤집히고, 뒤집혔는지 여부를 나타내는 'is_poison' 컬럼이 추가된 데이터프레임.
add_label_noise <- function(data, flip_frac_pos = 0, flip_frac_neg = 0) {
  if ((flip_frac_pos <= 0 && flip_frac_neg <= 0) || 
      any(c(flip_frac_pos, flip_frac_neg) > 1) ||
      any(c(flip_frac_pos, flip_frac_neg) < 0)) {
    data$is_poison <- FALSE
    return(data)
  }

  data$is_poison <- FALSE

  # 각 클래스별로 인덱스 추출
  pos_indices <- which(data$y == 1)
  neg_indices <- which(data$y == -1)

  # 각 클래스에서 뒤집을 샘플 수 계산
  n_flip_pos <- floor(length(pos_indices) * flip_frac_pos)
  n_flip_neg <- floor(length(neg_indices) * flip_frac_neg)

  # Positive 클래스 라벨 뒤집기
  if (n_flip_pos > 0) {
    indices_to_flip <- sample(pos_indices, n_flip_pos)
    data$y[indices_to_flip] <- -1  # 라벨 뒤집기 (1 -> -1)
    data$is_poison[indices_to_flip] <- TRUE
  }
  # Negative 클래스 라벨 뒤집기
  if (n_flip_neg > 0) {
    indices_to_flip <- sample(neg_indices, n_flip_neg)
    data$y[indices_to_flip] <- 1   # 라벨 뒤집기 (-1 -> 1)
    data$is_poison[indices_to_flip] <- TRUE
  }

  return(data)
}


## (c) w, b 추출 (loss_shifting 전용 포함) --------------------------------
get_wb <- function(model) {
  # ─ 1) e1071::svm -------------------------------------------------------
  if (inherits(model, "svm")) {
    w <- as.numeric(t(model$coefs) %*% model$SV)
    return(list(w = w, b = -model$rho))
  }
  # ─ 2) cv.glmnet --------------------------------------------------------
  if (inherits(model, "cv.glmnet")) {
    co <- as.numeric(coef(model, s = "lambda.min"))
    return(list(w = co[2:3], b = co[1]))
  }
  # ─ 3) loss_shifting 모델 (theta 벡터) -----------------------------------
  if (is.list(model) && "theta" %in% names(model)) {
    theta <- model$theta
    return(list(w = theta[-1], b = theta[1]))
  }
  # ─ 4) 혹시 모를 계층 구조 탐색 (최대 4단계) -----------------------------
  obj <- model
  for (step in 1:4) {
    if (is.list(obj) && "theta" %in% names(obj)) {
      th <- obj$theta; return(list(w = th[-1], b = th[1]))
    }
    if (is.list(obj) && "model" %in% names(obj))
      obj <- obj$model else break
  }
  NULL
}

## (d) 성능 지표 -----------------------------------------------------------
evaluate_metrics <- function(truth, pred) {
  cm <- table(factor(truth, levels = c(-1, 1)),
              factor(pred,  levels = c(-1, 1)))
  acc <- sum(diag(cm)) / sum(cm)
  prec <- if ((cm[2, 2] + cm[1, 2]) == 0) NA else cm[2, 2] / (cm[2, 2] + cm[1, 2])
  rec  <- if ((cm[2, 2] + cm[2, 1]) == 0) NA else cm[2, 2] / (cm[2, 2] + cm[2, 1])
  f1   <- if (is.na(prec) || is.na(rec) || (prec + rec) == 0) NA else
          2 * prec * rec / (prec + rec)
  tibble(accuracy = acc, f1 = f1)
}

# ========================================================================
# 3) 메인 함수 (수정됨) ----------------------------------------------------
run_and_visualize_final <- function(seed = 20250808,
                                    n_sample = 600,
                                    noise_type = c("regional", "label_flip"),
                                    poison_frac = 0.1,    # regional 노이즈용
                                    flip_frac_pos = 0.1,  # label_flip +1 클래스용
                                    flip_frac_neg = 0.1)  # label_flip -1 클래스용
{
  set.seed(seed)
  noise_type <- match.arg(noise_type)

  # ---------- (A) 데이터 생성 -------------------------------------------
  n_pos <- n_sample %/% 2; n_neg <- n_sample - n_pos
  pos   <- matrix(rnorm(2 * n_pos, 0.65, 0.5), ncol = 2)
  neg   <- matrix(rnorm(2 * n_neg, -0.65, 0.5), ncol = 2)
  raw   <- data.frame(X1 = c(pos[, 1], neg[, 1]),
                      X2 = c(pos[, 2], neg[, 2]),
                      y  = c(rep(1, n_pos), rep(-1, n_neg)))

  idx   <- sample.int(nrow(raw))
  test  <- raw[idx[1:round(0.3 * nrow(raw))], ]
  train <- raw[idx[-(1:round(0.3 * nrow(raw)))], ]

  # [수정됨] 노이즈 타입에 따라 다른 함수 호출
  if (noise_type == "regional") {
    cat(sprintf("Applying REGIONAL outlier noise (frac: %.2f)...\n", poison_frac))
    train <- add_regional_outliers(
               train,
               clusters = list(list(cx = -1, cy = -1, label = 1)),
               outlier_frac = ceiling(nrow(train) * poison_frac),
               sd = 0.1)
  } else if (noise_type == "label_flip") {
    cat(sprintf("Applying LABEL FLIP noise (Pos: %.2f, Neg: %.2f)...\n", 
                flip_frac_pos, flip_frac_neg))
    train <- add_label_noise(train, 
                             flip_frac_pos = flip_frac_pos, 
                             flip_frac_neg = flip_frac_neg)
  }

  # 스케일링 -------------------------------------------------------------
  mu <- colMeans(train[, 1:2]); sdv <- apply(train[, 1:2], 2, sd)
  train_s <- as.data.frame(scale(train[, 1:2], center = mu, scale = sdv))
  train_s$y <- train$y
  test_s  <- as.data.frame(scale(test[, 1:2],  center = mu, scale = sdv))
  test_s$y <- test$y
  
  # is_poison 컬럼 처리
  if (noise_type == "regional") {
      train_s$is_poison <- (train$X1 < -1.5 & train$X2 < -1.5)
  } else {
      train_s$is_poison <- train$is_poison
  }


  # ---------- (B) 모델 학습 (기존과 동일) --------------------------------
  results <- list()

  # ① Loss-Shift
  combos <- expand.grid(base_loss = c("hinge","sqhinge","logistic","exp"),
                        style     = c("none","soft","hard"),
                        stringsAsFactors = FALSE)
  for (i in seq_len(nrow(combos))) {
    p   <- combos[i, ]
    tag <- sprintf("LossShift_%s_%s_linear", p$base_loss, p$style)
    message("Training ", tag, " ...")
    results[[tag]] <- tryCatch({
      mod_wrap <- train_loss_shift(
        X_train = as.matrix(train_s[, 1:2]), y_train = train_s$y,
        base_loss = p$base_loss, style = p$style, kernel = "linear",
        lambda_grid = 2^(-5:3), alpha_grid = c(0.5, 1, 2),
        eta_grid = c(0.5, 1, 2), n_folds = 3, restarts = 3, verbose = FALSE)
      pred <- predict_loss_shift(mod_wrap$model, as.matrix(test_s[, 1:2]))
      list(model = mod_wrap$model, metrics = evaluate_metrics(test_s$y, pred))
    }, error = function(e) { message("  -> ERROR: ", e$message); NULL })
  }

  # ② e1071 SVM
  message("Training e1071_linear ...")
  results[["e1071_linear"]] <- tryCatch({
    best <- tune(e1071::svm, train.x = as.matrix(train_s[, 1:2]),
                 train.y = factor(train_s$y), kernel = "linear",
                 cost = 2^(-1:1), tunecontrol = tune.control(cross = 3))$best.model
    pred <- as.numeric(as.character(predict(best, as.matrix(test_s[, 1:2]))))
    list(model = best, metrics = evaluate_metrics(test_s$y, pred))
  }, error = function(e) { message("  -> ERROR: ", e$message); NULL })

  # ③ GLMNet
  message("Training GLMNet_linear ...")
  results[["GLMNet_linear"]] <- tryCatch({
    cvfit <- cv.glmnet(as.matrix(train_s[, 1:2]), ifelse(train_s$y == 1, 1, 0),
                       family = "binomial", alpha = 1, nfolds = 3, standardize = FALSE)
    prob <- as.vector(predict(cvfit, as.matrix(test_s[, 1:2]),
                              s = "lambda.min", type = "response"))
    pred <- ifelse(prob >= 0.5, 1, -1)
    list(model = cvfit, metrics = evaluate_metrics(test_s$y, pred))
  }, error = function(e) { message("  -> ERROR: ", e$message); NULL })

  # ---------- (C) 성능 요약 (기존과 동일) --------------------------------
  cat("\n## Performance Metrics (accuracy, f1)\n")
  metrics_df <- bind_rows(lapply(names(results), function(nm) {
    if (!is.null(results[[nm]]$metrics))
      mutate(as_tibble(results[[nm]]$metrics), model = nm)
  }))
  if (nrow(metrics_df) > 0) print(arrange(metrics_df, desc(f1)))
  else message("No metrics available.")

  # ---------- (D) 시각화 (수정됨) --------------------------------------
  base_losses <- c("hinge", "sqhinge", "logistic", "exp")
  for (loss in base_losses) {
    message("\n▶ Generating plot for '", loss, "' loss ...")
    models <- c("e1071_linear", "GLMNet_linear",
                sprintf("LossShift_%s_none_linear", loss),
                sprintf("LossShift_%s_soft_linear", loss),
                sprintf("LossShift_%s_hard_linear", loss))

    bd <- do.call(bind_rows, lapply(models, function(m) {
      if (is.null(results[[m]])) return(NULL)
      wb <- get_wb(results[[m]]$model)
      if (is.null(wb)) return(NULL)
      w <- wb$w; b <- wb$b; xr <- range(train_s$X1)
      if (abs(w[2]) < 1e-8) {
        tibble(model = m, x1 = NA_real_, x2 = NA_real_, x_int = -b / w[1])
      } else {
        x1v <- xr; x2v <- (-b - w[1] * x1v) / w[2]
        tibble(model = m, x1 = x1v, x2 = x2v, x_int = NA_real_)
      }
    }))

    if (nrow(bd) == 0) { message("   -> No valid boundaries."); next }

    cols <- c("-1" = "gray70", "1" = "gray30", "e1071_linear" = "#00BFC4",
              "GLMNet_linear" = "#F8766D")
    cols[sprintf("LossShift_%s_none_linear", loss)] <- "#7CAE00"
    cols[sprintf("LossShift_%s_soft_linear", loss)] <- "#C77CFF"
    cols[sprintf("LossShift_%s_hard_linear", loss)] <- "#000000"
    
    # [수정됨] 부제 텍스트 동적 생성
    subtitle_text <- if (noise_type == "regional") {
      sprintf("Regional Poison Fraction: %.2f", poison_frac)
    } else {
      sprintf("Label Flip Fractions -> Pos(+1): %.2f, Neg(-1): %.2f", flip_frac_pos, flip_frac_neg)
    }

    p <- ggplot() +
      geom_point(data = train_s, aes(x = X1, y = X2, colour = factor(y), shape = is_poison),
                 alpha = 0.6, size = 2.5) +
      geom_line(data = filter(bd, !is.na(x2)), aes(x = x1, y = x2, colour = model),
                linewidth = 1.1) +
      geom_vline(data = filter(bd, is.na(x2)), aes(xintercept = x_int, colour = model),
                 linewidth = 1.1, linetype = "dashed") +
      scale_shape_manual(values = c("TRUE" = 17, "FALSE" = 16), guide = "none") +
      scale_colour_manual("Model / Class", values = cols, limits = names(cols)) +
      labs(title = sprintf("Decision Boundary (%s): '%s' Loss", noise_type, toupper(loss)),
           subtitle = subtitle_text,
           x = "Feature 1 (scaled)", y = "Feature 2 (scaled)") +
      theme_bw(14) +
      coord_cartesian(xlim = range(train_s$X1) * 1.1, ylim = range(train_s$X2) * 1.1)
    print(p)
  }
}

```

## plot

### Case 1: 지역적 아웃라이어 (Regional Outlier)

```{r, results='hide'}
# Case 1: 지역적 아웃라이어 (Regional Outlier)

run_and_visualize_final(seed = 10, noise_type = "regional", poison_frac = 0.15)
```

### Case 2: 대칭적인 라벨 뒤집기 (Symmetric Label Flipping)

```{r, results='hide'}
# Case 2: 대칭적인 라벨 뒤집기 (Symmetric Label Flipping)

run_and_visualize_final(seed = 42, noise_type = "label_flip", 
                         flip_frac_pos = 0.1, flip_frac_neg = 0.1)
```

### Case 3: 비대칭적인 라벨 뒤집기 (Asymmetric Label Flipping)

```{r, results='hide'}
# Case 3: 비대칭적인 라벨 뒤집기 (Asymmetric Label Flipping)

run_and_visualize_final(seed = 42, noise_type = "label_flip", 
                        flip_frac_pos = 0.15, flip_frac_neg = 0.05)
```

