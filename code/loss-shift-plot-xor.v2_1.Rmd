---
title: "loss-shift-plot-xor.v2_1"
author: "Taenyoung Lee"
date: "`r format(Sys.time(), '%Y년 %B %d일')`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: flatly
    code_folding: hide
---

## test function

```{r, results='hide'}


# 0) 라이브러리 -----------------------------------------------------------
suppressPackageStartupMessages({
  library(e1071)
  library(dplyr);  library(tibble);  library(ggplot2);  library(tidyr)
})

# 1) Loss-Shift 함수 로드 --------------------------------------------------

source("loss_shifting_v2_1.R")  # train_loss_shift(), predict_loss_shift(), ...

# 2) 유틸리티 --------------------------------------------------------------

## 지역(outlier) 생성
add_regional_outliers <- function(data, clusters, outlier_frac, sd = 0.05) {
  if (!"is_poison" %in% names(data)) data$is_poison <- FALSE
  dfs <- list(data)
  for (cl in clusters) {
    n_extra <- if (outlier_frac >= 1) as.integer(outlier_frac) else
      ceiling(sum(data$y == cl$label, na.rm = TRUE) * outlier_frac)
    if (n_extra > 0) {
      pts <- matrix(rnorm(2 * n_extra, 0, sd), ncol = 2)
      dfs[[length(dfs) + 1]] <- data.frame(
        X1 = pts[, 1] + cl$cx,
        X2 = pts[, 2] + cl$cy,
        y  = cl$label,
        is_poison = TRUE
      )
    }
  }
  out <- bind_rows(dfs)
  out$is_poison[is.na(out$is_poison)] <- FALSE
  out
}

## 라벨 노이즈
add_label_noise <- function(data, flip_frac_pos = 0, flip_frac_neg = 0) {
  if ((flip_frac_pos <= 0 && flip_frac_neg <= 0) ||
      any(c(flip_frac_pos, flip_frac_neg) > 1) ||
      any(c(flip_frac_pos, flip_frac_neg) < 0)) {
    data$is_poison <- FALSE
    return(data)
  }
  data$is_poison <- FALSE
  pos_indices <- which(data$y == 1)
  neg_indices <- which(data$y == -1)
  n_flip_pos <- floor(length(pos_indices) * flip_frac_pos)
  n_flip_neg <- floor(length(neg_indices) * flip_frac_neg)
  if (n_flip_pos > 0) {
    indices_to_flip <- sample(pos_indices, n_flip_pos)
    data$y[indices_to_flip] <- -1
    data$is_poison[indices_to_flip] <- TRUE
  }
  if (n_flip_neg > 0) {
    indices_to_flip <- sample(neg_indices, n_flip_neg)
    data$y[indices_to_flip] <- 1
    data$is_poison[indices_to_flip] <- TRUE
  }
  data
}

## 성능 지표
evaluate_metrics <- function(truth, pred) {
  cm <- table(factor(truth, levels = c(-1, 1)),
              factor(pred,  levels = c(-1, 1)))
  acc <- sum(diag(cm)) / sum(cm)
  prec <- if ((cm[2, 2] + cm[1, 2]) == 0) NA else cm[2, 2] / (cm[2, 2] + cm[1, 2])
  rec  <- if ((cm[2, 2] + cm[2, 1]) == 0) NA else cm[2, 2] / (cm[2, 2] + cm[2, 1])
  f1   <- if (is.na(prec) || is.na(rec) || (prec + rec) == 0) NA else
    2 * prec * rec / (prec + rec)
  tibble(accuracy = acc, f1 = f1)
}

## XOR 데이터 (I:+1, II:-1, III:+1, IV:-1)
make_xor_data <- function(n_total = 600, mu = 1, sd = 0.35) {
  n_per <- rep(n_total %/% 4, 4); n_per[seq_len(n_total %% 4)] <- n_per[seq_len(n_total %% 4)] + 1
  centers <- tibble::tibble(
    cx = c( mu, -mu, -mu,  mu),
    cy = c( mu, -mu,  mu, -mu),
    y  = c(  1,  1,   -1,  -1)  # XOR 패턴
  )
  dfs <- lapply(1:4, function(i) {
    pts <- matrix(rnorm(2 * n_per[i], 0, sd), ncol = 2)
    data.frame(X1 = pts[,1] + centers$cx[i],
               X2 = pts[,2] + centers$cy[i],
               y  = centers$y[i])
  })
  d <- dplyr::bind_rows(dfs)
  d$y <- as.integer(d$y)
  d
}

# 3) 메인 함수 -------------------------------------------------------------
run_and_visualize_final <- function(seed = 2025,
                                    n_sample = 600,
                                    noise_type = c("regional", "label_flip"),
                                    poison_frac = 0.1,
                                    flip_frac_pos = 0.1,
                                    flip_frac_neg = 0.1,
                                    xor_mu = 1, xor_sd = 0.35,
                                    loss_shift_restarts = 3,
                                    loss_shift_folds = 3,
                                    loss_shift_seed = NULL) {

  set.seed(seed)
  noise_type <- match.arg(noise_type)

  # ----- (A) XOR 데이터 생성 ---------------------------------------------
  raw <- make_xor_data(n_total = n_sample, mu = xor_mu, sd = xor_sd)
  idx   <- sample.int(nrow(raw))
  test  <- raw[idx[1:round(0.3 * nrow(raw))], ]
  train <- raw[idx[-(1:round(0.3 * nrow(raw)))], ]

  # 노이즈 주입
  if (noise_type == "regional") {
    cat(sprintf("Applying REGIONAL outlier noise (frac: %.2f) for XOR...\n", poison_frac))
    n_each <- ceiling(nrow(train) * poison_frac / 2)
    train <- add_regional_outliers(
      train,
      clusters = list(
        list(cx = -xor_mu, cy =  xor_mu, label =  1),  # II에 +1 주입 (원래 -1)
        list(cx =  xor_mu, cy = -xor_mu, label =  1)   # IV에 +1 주입 (원래 -1)
      ),
      outlier_frac = n_each, sd = 0.20
    )
  } else if (noise_type == "label_flip") {
    cat(sprintf("Applying LABEL FLIP noise (Pos: %.2f, Neg: %.2f)...\n",
                flip_frac_pos, flip_frac_neg))
    train <- add_label_noise(train,
                             flip_frac_pos = flip_frac_pos,
                             flip_frac_neg = flip_frac_neg)
  }

  # 스케일링(시각화/타 모델용) — Loss-Shift는 내부 표준화가 있어도 중복 안전
  mu <- colMeans(train[, 1:2]); sdv <- apply(train[, 1:2], 2, sd)
  train_s <- as.data.frame(scale(train[, 1:2], center = mu, scale = sdv))
  train_s$y <- train$y
  test_s  <- as.data.frame(scale(test[, 1:2],  center = mu, scale = sdv))
  test_s$y <- test$y
  train_s$is_poison <- if ("is_poison" %in% names(train)) train$is_poison else FALSE

  # ----- (B) 모델 학습 ----------------------------------------------------
  results <- list()

  # ① Loss-Shift (kernel = "gaussian")
  combos <- expand.grid(base_loss = c("hinge","sqhinge","logistic","exp"),
                        style     = c("none","soft","hard"),
                        stringsAsFactors = FALSE)
  for (i in seq_len(nrow(combos))) {
    p   <- combos[i, ]
    tag <- sprintf("LossShift_%s_%s_rbf", p$base_loss, p$style)
    message("Training ", tag, " ...")
    results[[tag]] <- tryCatch({
      mod_wrap <- train_loss_shift(
        X_train = as.matrix(train_s[, 1:2]), y_train = train_s$y,
        base_loss = p$base_loss, style = p$style, kernel = "gaussian",
        lambda_grid = 2^(-5:3), alpha_grid = c(0.5, 1, 2), eta_grid = c(0.5, 1, 2),
        n_folds = loss_shift_folds, restarts = loss_shift_restarts,
        svm_dual = TRUE, verbose = FALSE,
        seed = loss_shift_seed)
      pred <- predict_loss_shift(mod_wrap$model, as.matrix(test_s[, 1:2]))
      list(model = mod_wrap$model,
           predict_fun = function(X) predict_loss_shift(mod_wrap$model, X),
           metrics = evaluate_metrics(test_s$y, pred))
    }, error = function(e) { message("  -> ERROR: ", e$message); NULL })
  }

  # ② e1071 SVM (RBF)
  message("Training e1071_rbf ...")
  results[["e1071_rbf"]] <- tryCatch({
    grdC <- 2^(-1:3); grdG <- 2^(-4:1)
    best <- tune(e1071::svm,
                 train.x = as.matrix(train_s[, 1:2]),
                 train.y = factor(train_s$y),
                 kernel = "radial", cost = grdC, gamma = grdG,
                 scale = FALSE,                      # 이미 스케일됨
                 tunecontrol = tune.control(cross = 3))$best.model
    pred <- as.numeric(as.character(predict(best, as.matrix(test_s[, 1:2]))))
    list(model = best,
         predict_fun = function(X) as.numeric(as.character(predict(best, X))),
         metrics = evaluate_metrics(test_s$y, pred))
  }, error = function(e) { message("  -> ERROR: ", e$message); NULL })

  # ----- (C) 성능 요약 ----------------------------------------------------
  cat("\n## Performance Metrics (accuracy, f1)\n")
  metrics_df <- bind_rows(lapply(names(results), function(nm) {
    if (!is.null(results[[nm]]$metrics))
      mutate(as_tibble(results[[nm]]$metrics), model = nm)
  }))
  if (nrow(metrics_df) > 0) print(arrange(metrics_df, desc(f1)))
  else message("No metrics available.")

  # ----- (D) 시각화: 등고선(결정경계) -------------------------------------
  x_seq <- seq(min(train_s$X1), max(train_s$X1), length.out = 200)
  y_seq <- seq(min(train_s$X2), max(train_s$X2), length.out = 200)
  grid  <- expand.grid(X1 = x_seq, X2 = y_seq)

  base_losses <- c("hinge", "sqhinge", "logistic", "exp")
  for (loss in base_losses) {
    message("\n▶ Generating contour for '", loss, "' loss ...")
    models <- c("e1071_rbf",
                sprintf("LossShift_%s_none_rbf", loss),
                sprintf("LossShift_%s_soft_rbf", loss),
                sprintf("LossShift_%s_hard_rbf", loss))

    # (D) 시각화: 등고선(결정경계) 부분 수정

    gp_list <- lapply(models, function(m) {
      if (is.null(results[[m]])) return(NULL) # predict_fun 존재 여부 체크는 불필요
    
      z <- if (grepl("e1071", m)) {
        # e1071 모델: decision.values = TRUE로 설정하여 결정 점수 획득
        pred_obj <- predict(results[[m]]$model, as.matrix(grid), decision.values = TRUE)
        # e1071은 종종 factor 레벨 순서에 따라 부호가 반대일 수 있으므로 조정
        as.numeric(attr(pred_obj, "decision.values")) * ifelse(results[[m]]$model$levels[1] == "1", -1, 1)
      } else {
        # LossShift 모델: 새로 추가한 type = "score" 옵션 사용
        predict_loss_shift(results[[m]]$model, as.matrix(grid), type = "score")
      }
      
      if (is.null(z)) return(NULL)
      tibble(model = m, X1 = grid$X1, X2 = grid$X2, z = as.numeric(z))
    })
    gp <- bind_rows(gp_list)
    if (nrow(gp) == 0) { message("  -> No valid predictions for contours."); next }

    cols <- c("-1" = "gray70", "1" = "gray30",
              "e1071_rbf" = "#00BFC4")
    cols[sprintf("LossShift_%s_none_rbf", loss)] <- "#7CAE00"
    cols[sprintf("LossShift_%s_soft_rbf", loss)] <- "#C77CFF"
    cols[sprintf("LossShift_%s_hard_rbf", loss)] <- "#000000"

    subtitle_text <- if (noise_type == "regional") {
      sprintf("Regional Poison Fraction: %.2f (XOR)", poison_frac)
    } else {
      sprintf("Label Flip Fractions -> Pos(+1): %.2f, Neg(-1): %.2f (XOR)", flip_frac_pos, flip_frac_neg)
    }

    p <- ggplot() +
      geom_point(data = train_s,
                 aes(x = X1, y = X2, colour = factor(y), shape = is_poison),
                 alpha = 0.6, size = 2.5) +
      geom_contour(data = gp,
                   aes(x = X1, y = X2, z = z, colour = model, group = model),
                   breaks = 0, linewidth = 1.1) +
      scale_shape_manual(values = c("TRUE" = 17, "FALSE" = 16), guide = "none") +
      scale_colour_manual("Model / Class", values = cols, limits = names(cols)) +
      labs(title = sprintf("Decision Boundary (%s, XOR): '%s' Loss (RBF)", noise_type, toupper(loss)),
           subtitle = subtitle_text,
           x = "Feature 1 (scaled)", y = "Feature 2 (scaled)") +
      theme_bw(14) +
      coord_cartesian(xlim = range(train_s$X1) * 1.1, ylim = range(train_s$X2) * 1.1)
    print(p)
  }
}

```

## plot

### Case 1: 지역적 아웃라이어 (Regional Outlier)

```{r, results='hide'}
# Case 1: 지역적 아웃라이어 (XOR)
run_and_visualize_final(seed = 10, noise_type = "regional", poison_frac = 0.15)
```

### Case 2: 대칭적인 라벨 뒤집기 (Symmetric Label Flipping)

```{r, results='hide'}
# Case 2: 대칭 라벨 플립 (XOR)
run_and_visualize_final(seed = 42, noise_type = "label_flip",
                        flip_frac_pos = 0.1, flip_frac_neg = 0.1)
```

### Case 3: 비대칭적인 라벨 뒤집기 (Asymmetric Label Flipping)

```{r, results='hide'}
# Case 3: 비대칭 라벨 플립 (XOR)
run_and_visualize_final(seed = 42, noise_type = "label_flip",
                        flip_frac_pos = 0.15, flip_frac_neg = 0.05)
```

### Case 4: 비대칭적인 라벨 뒤집기 강화 (Reinforce Asymmetric Label Flipping)

```{r, results='hide'}
# Case 4 강화 (XOR)
run_and_visualize_final(seed = 42, noise_type = "label_flip",
                        flip_frac_pos = 0.3, flip_frac_neg = 0.05)
```