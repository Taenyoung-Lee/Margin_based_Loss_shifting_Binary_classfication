---
title: "xor"
author: "Taenyong_Lee"
date: "2025-09-16"
output: html_document
---

  

```{r}
## ───────────────────────────────────────────────────────────────────
## 의존성 패키지 로드
## ───────────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  # library(e1071)  # [CHANGED] 더 이상 사용하지 않음
  library(tidyr)
  library(dplyr)
  library(knitr)
  library(kableExtra)
  library(future)
  library(future.apply)
  library(progressr)
  library(glmnet)
  library(kernlab)  # [CHANGED] ksvm 사용
})

## ───────────────────────────────────────────────────────────────────
## 1) 표 컬럼 정의
## ───────────────────────────────────────────────────────────────────
TABLE_COLS <- c(
  "n","p","rate_pos","rate_neg","true",
  "hinge_none","sqhinge_none","logistic_none","exp_none",
  "hinge_soft","sqhinge_soft","logistic_soft","exp_soft",
  "hinge_hard","sqhinge_hard","logistic_hard","exp_hard",
  "ksvm", "glm"  # [CHANGED] e1071 → ksvm
)
MODEL_COLS <- setdiff(TABLE_COLS, c("n","p","rate_pos","rate_neg","true"))
acc <- function(y_true, y_pred) mean(y_true == y_pred)

## ───────────────────────────────────────────────────────────────────
## 2~4) Helper Functions
## ───────────────────────────────────────────────────────────────────

# 데이터 생성 함수 (기존과 동일)
gen_xor_2d <- function(n, p = 2, mu = 1, sd = 0.5, seed = NULL) {
  stopifnot(p == 2)
  if (!is.null(seed)) set.seed(seed)
  n_per_quad <- n %/% 4; n_rem <- n %% 4
  ns <- rep(n_per_quad, 4) + c(rep(1, n_rem), rep(0, 4 - n_rem))
  X_p1 <- cbind(rnorm(ns[1], mean=mu, sd=sd), rnorm(ns[1], mean=mu, sd=sd))
  X_p2 <- cbind(rnorm(ns[2], mean=-mu, sd=sd), rnorm(ns[2], mean=-mu, sd=sd))
  X_n1 <- cbind(rnorm(ns[3], mean=mu, sd=sd), rnorm(ns[3], mean=-mu, sd=sd))
  X_n2 <- cbind(rnorm(ns[4], mean=-mu, sd=sd), rnorm(ns[4], mean=mu, sd=sd))
  X <- rbind(X_p1, X_p2, X_n1, X_n2); y <- c(rep(1, ns[1]+ns[2]), rep(-1, ns[3]+ns[4]))
  shuffle_idx <- sample(n); list(X = X[shuffle_idx, ], y = y[shuffle_idx])
}

# 레이블 노이즈 함수 (기존과 동일)
flip_labels_asym <- function(y, rate_pos = 0, rate_neg = 0, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  idx_p <- which(y==1); kp <- floor(length(idx_p)*rate_pos)
  idx_n <- which(y==-1); kn <- floor(length(idx_n)*rate_neg)
  flip_idx <- c(if(kp>0) sample(idx_p,kp) else integer(0), if(kn>0) sample(idx_n,kn) else integer(0))
  y2 <- y; if(length(flip_idx)) y2[flip_idx] <- -y2[flip_idx]; list(y=y2, flipped_idx=flip_idx)
}

# [MODIFIED] 표준화 유틸 (입출력 동일)
.standardize_fit <- function(X) {
  mu <- colMeans(X)
  sd <- apply(X, 2, sd); sd[sd == 0] <- 1
  Xs <- sweep(sweep(X, 2, mu, "-"), 2, sd, "/")
  list(Xs = Xs, mu = mu, sd = sd)
}
.standardize_apply <- function(X, mu, sd) {
  sd[sd == 0] <- 1
  sweep(sweep(X, 2, mu, "-"), 2, sd, "/")
}

## ───────────────────────────────────────────────────────────────────
## [CHANGED] e1071 대체: kernlab::ksvm 기반 예측기
## ───────────────────────────────────────────────────────────────────
# - kernel = "radial" 이면 rbfdot( gamma = 1/(2*sigma^2) ) 사용
# - kernel = "linear" 이면 vanilladot 사용
# - 3-fold CV로 (sigma, C) 그리드 탐색 (동률: 큰 C, 큰 sigma 우선)
pred_ksvm_aligned <- function(Xtr, ytr, Xte, kernel = "radial", seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  n <- nrow(Xtr)
  y_fac <- factor(ifelse(ytr > 0, "pos", "neg"), levels = c("neg","pos"))

  # 공통 그리드
  if (kernel == "radial") {
    if (!exists(".sigma_grid_from_median_scaled")) {
      stop(".sigma_grid_from_median_scaled()가 필요합니다 (loss_shifting_v5.R 로드 여부 확인).")
    }
    sigma_grid <- .sigma_grid_from_median_scaled(Xtr, sigma_mult = c(0.5, 1, 2), seed = seed)
  } else {
    sigma_grid <- NA_real_
  }
  lambda_grid <- 10^seq(-6, -3, length.out = 4)
  C_grid <- 1 / ( n * lambda_grid)

  # CV 폴드
  if (!exists("cv_split")) stop("cv_split()가 필요합니다 (loss_shifting_v5.R).")
  folds <- cv_split(n, n_folds = 3, strat_y = ytr, seed = seed)

  if (kernel == "radial") {
    grid <- expand.grid(sigma = sigma_grid, C = C_grid, KEEP.OUT.ATTRS = FALSE)
  } else {
    grid <- data.frame(sigma = NA_real_, C = C_grid)
  }

  cv_accs <- numeric(nrow(grid))
  for (i in seq_len(nrow(grid))) {
    Cval <- grid$C[i]
    if (kernel == "radial") {
      s     <- grid$sigma[i]
      gamma <- 1 / (2 * s^2)
      kobj  <- rbfdot(gamma = gamma)
    } else {
      kobj  <- vanilladot()
    }

    fold_acc <- numeric(length(folds))
    for (k in seq_along(folds)) {
      tr_idx <- folds[[k]]$tr
      va_idx <- folds[[k]]$va

      fit <- kernlab::ksvm(
        x = Xtr[tr_idx, , drop = FALSE],
        y = y_fac[tr_idx],
        type = "C-svc",
        kernel = kobj,
        C = Cval,
        scaled = FALSE
      )
      pr_va <- predict(fit, Xtr[va_idx, , drop = FALSE])
      y_hat <- ifelse(pr_va == "pos", 1L, -1L)
      fold_acc[k] <- mean(y_hat == ifelse(ytr[va_idx] > 0, 1L, -1L))
    }
    cv_accs[i] <- mean(fold_acc)
  }

  # tie-break: 큰 C, 큰 sigma
  best_idx <- which(cv_accs == max(cv_accs))
  if (length(best_idx) > 1) {
    G <- grid[best_idx, , drop = FALSE]
    if (kernel == "radial") o <- order(-G$C, -G$sigma) else o <- order(-G$C)
    best_idx <- best_idx[o[1]]
  }

  best_C <- grid$C[best_idx]
  if (kernel == "radial") {
    best_sigma <- grid$sigma[best_idx]
    best_gamma <- 1 / (2 * best_sigma^2)
    best_kobj  <- rbfdot(gamma = best_gamma)
  } else {
    best_kobj  <- vanilladot()
  }

  fitF <- kernlab::ksvm(
    x = Xtr, y = y_fac,
    type = "C-svc",
    kernel = best_kobj,
    C = best_C,
    scaled = FALSE
  )
  pr_te <- predict(fitF, Xte)
  ifelse(pr_te == "pos", 1L, -1L)
}

## ───────────────────────────────────────────────────────────────────
## (그대로 유지) GLM 경로: 커널-GLM with glmnet
## ───────────────────────────────────────────────────────────────────
pred_glm_kernel_glmnet <- function(Xtr, ytr, Xte, seed = NULL) {
  if (!requireNamespace("glmnet", quietly = TRUE))
    stop("glmnet 패키지가 필요합니다. install.packages('glmnet')")
  if (!exists(".sigma_grid_from_median_scaled"))
    stop(".sigma_grid_from_median_scaled()가 필요합니다 (loss_shifting_v5.R).")
  if (!exists("cv_split"))
    stop("cv_split()가 필요합니다 (loss_shifting_v5.R).")

  rbfK <- if (exists("rbf_kernel")) {
    function(A, B = NULL, sigma) rbf_kernel(A, B, sigma)
  } else {
    function(A, B = NULL, sigma) {
      if (is.null(B)) B <- A
      A <- as.matrix(A); B <- as.matrix(B)
      D2 <- outer(rowSums(A^2), rowSums(B^2), "+") - 2 * tcrossprod(A, B)
      exp(-D2 / (2 * sigma^2))
    }
  }

  if (!is.null(seed)) set.seed(seed)

  sigma_grid  <- .sigma_grid_from_median_scaled(Xtr, sigma_mult = c(0.5, 1, 2), seed = seed)
  lambda_grid <- 10^seq(-6, -3, length.out = 4)
  cv_params   <- expand.grid(sigma = sigma_grid, lambda = lambda_grid, KEEP.OUT.ATTRS = FALSE)
  folds       <- cv_split(nrow(Xtr), n_folds = 3, strat_y = ytr, seed = seed)

  make_Z_train <- function(K_tr) {
    ee  <- eigen(K_tr, symmetric = TRUE)
    val <- pmax(ee$values, .Machine$double.eps)
    U   <- ee$vectors
    Z_tr <- sweep(U, 2, sqrt(val), `*`)
    U_div <- sweep(U, 2, sqrt(val), `/`)
    list(Z_tr = Z_tr, U_div = U_div, val = val)
  }
  make_Z_new <- function(K_new, U_div) K_new %*% U_div

  cv_accs <- numeric(nrow(cv_params))
  for (i in seq_len(nrow(cv_params))) {
    s   <- cv_params$sigma[i]
    lam <- cv_params$lambda[i]
    fold_accs <- numeric(length(folds))

    for (k in seq_along(folds)) {
      tr_idx <- folds[[k]]$tr
      va_idx <- folds[[k]]$va

      X_tr <- Xtr[tr_idx, , drop = FALSE]
      y_tr <- ytr[tr_idx]
      X_va <- Xtr[va_idx, , drop = FALSE]
      y_va <- ytr[va_idx]

      K_tr  <- rbfK(X_tr, sigma = s)
      Zinfo <- make_Z_train(K_tr)
      Z_tr  <- Zinfo$Z_tr
      U_div <- Zinfo$U_div

      y_tr_01 <- ifelse(y_tr == 1, 1L, 0L)
      fit <- glmnet::glmnet(
        x = Z_tr, y = y_tr_01,
        family = "binomial",
        alpha = 0,
        lambda = lam,
        intercept = TRUE,
        standardize = FALSE
      )

      K_va_tr <- rbfK(X_va, X_tr, sigma = s)
      Z_va    <- make_Z_new(K_va_tr, U_div)
      p_va    <- as.numeric(glmnet::predict.glmnet(fit, newx = Z_va, type = "response", s = lam))
      y_hat   <- ifelse(p_va >= 0.5, 1L, -1L)

      fold_accs[k] <- mean(y_hat == y_va)
    }
    cv_accs[i] <- mean(fold_accs)
  }

  best_idx <- which(cv_accs == max(cv_accs))
  if (length(best_idx) > 1) {
    G <- cv_params[best_idx, , drop = FALSE]
    o <- order(-G$lambda, -G$sigma)  # 큰 λ ▶ 큰 σ
    best_idx <- best_idx[o[1]]
  }
  best_sigma  <- cv_params$sigma[best_idx]
  best_lambda <- cv_params$lambda[best_idx]

  K_full  <- rbfK(Xtr, sigma = best_sigma)
  ZinfoF  <- make_Z_train(K_full)
  Z_full  <- ZinfoF$Z_tr
  U_div_F <- ZinfoF$U_div

  ytr_01 <- ifelse(ytr == 1, 1L, 0L)
  fitF <- glmnet::glmnet(
    x = Z_full, y = ytr_01,
    family = "binomial",
    alpha = 0,
    lambda = best_lambda,
    intercept = TRUE,
    standardize = FALSE
  )

  K_te_tr <- rbfK(Xte, Xtr, sigma = best_sigma)
  Z_te    <- make_Z_new(K_te_tr, U_div_F)
  p_te    <- as.numeric(glmnet::predict.glmnet(fitF, newx = Z_te, type = "response", s = best_lambda))
  ifelse(p_te >= 0.5, 1L, -1L)
}

## ───────────────────────────────────────────────────────────────────
## [CHANGED] 모델 실행 함수: loss_shift에 precomp 전달
## ───────────────────────────────────────────────────────────────────
run_single_model <- function(model_name, Xtr_raw, ytr, Xte_raw, Xtr_s, Xte_s, kernel,
                             precomp = NULL, seed = NULL) {
  switch(model_name,
    "ksvm" = pred_ksvm_aligned(Xtr_s, ytr, Xte_s,
                               kernel = ifelse(kernel=="radial","radial","linear"),
                               seed = seed),
    "glm"  = pred_glm_kernel_glmnet(Xtr_s, ytr, Xte_s, seed = seed),
    {
      parts <- strsplit(model_name, "_")[[1]]
      res <- train_loss_shift(
        X_train = Xtr_raw, y_train = ytr, X_test = Xte_raw,
        base_loss = parts[1], style = parts[2], kernel = "gaussian",
        restarts = 10, n_folds = 3, svm_dual = TRUE, max_iter = 100,
        line_search = TRUE, verbose = FALSE,
        precomp = precomp,          # <<< precomp 전달
        seed = seed
      )
      res$test_pred
    }
  )
}

## ───────────────────────────────────────────────────────────────────
## 5) 시뮬레이션 핵심 함수: precomp를 한 번만 생성 후 공유
## ───────────────────────────────────────────────────────────────────
run_grid_fully_parallel <- function(n_train, n_test, rates_pos, rates_neg,
                                    data_gen_fun, oracle_fun, kernel = "linear",
                                    k = 20, seed = 2025, digits = 5) {
  p <- 2
  plan(multisession)

  task_grid <- expand.grid(
    rate_pos = rates_pos,
    rate_neg = rates_neg,
    rep_i = 1:k,
    stringsAsFactors = FALSE
  )
  
  cat(sprintf("\n--- Running a total of %d independent simulations in parallel ---\n", nrow(task_grid)))
  
  with_progress({
    p_bar <- progressor(steps = nrow(task_grid))
    
    results_list <- future_lapply(1:nrow(task_grid), function(j) {
      current_rate_pos <- task_grid$rate_pos[j]
      current_rate_neg <- task_grid$rate_neg[j]
      current_seed <- seed + j
      
      # 데이터 생성
      Tr <- data_gen_fun(n_train, seed = current_seed + 100)
      Te <- data_gen_fun(n_test,  seed = current_seed + 200)
      
      # 표준화
      st <- .standardize_fit(Tr$X)
      Tr_X_s <- st$Xs
      Te_X_s <- .standardize_apply(Te$X, st$mu, st$sd)

      # (중요) loss_shift용 precomp를 "한 번"만 생성
      #  - sigma_grid를 NULL로 두면 v5 내부의 median heuristic + sigma_mult 사용
      precomp <- loss_shift_build_precomp(
        X = Tr$X, kernel = "gaussian", standardize = TRUE,
        sigma_grid = NULL, sigma_mult = c(0.5, 1, 2),
        seed = current_seed + 10
      )

      # 노이즈 추가
      ytr_noisy <- flip_labels_asym(
        Tr$y, rate_pos = current_rate_pos, rate_neg = current_rate_neg,
        seed = current_seed + 3
      )$y
      
      # 모델 학습/평가 (loss_shift 계열은 precomp 공유)
      accuracies <- sapply(MODEL_COLS, function(model_name) {
        y_pred <- run_single_model(
          model_name,
          Tr$X,            # raw train
          ytr_noisy,
          Te$X,            # raw test
          Tr_X_s,          # standardized train
          Te_X_s,          # standardized test
          kernel,
          precomp = precomp,   # <<< 공유 precomp
          seed = current_seed + 100
        )
        acc(Te$y, y_pred)
      })
      true_acc <- acc(Te$y, oracle_fun(Te$X))
      
      p_bar(sprintf("rp=%.2f", current_rate_pos))
      
      c(rate_pos = current_rate_pos, rate_neg = current_rate_neg, accuracies, true = true_acc)
      
    }, future.seed = TRUE)
  })

  # 결과 집계 및 포맷팅
  cat("\n--- All simulations complete. Aggregating results. ---\n")
  full_results_df <- dplyr::bind_rows(lapply(results_list, as.list))
  agg_results <- full_results_df %>%
    dplyr::group_by(rate_pos, rate_neg) %>%
    dplyr::summarise(dplyr::across(all_of(c(MODEL_COLS, "true")), list(mean = mean, sd = sd)), .groups = 'drop')

  formatted_df <- data.frame(n = n_train, p = p, rate_pos = agg_results$rate_pos, rate_neg = agg_results$rate_neg)
  all_cols_ordered <- TABLE_COLS[-(1:4)]
  
  for (col in all_cols_ordered) {
    mean_col <- paste0(col, "_mean"); sd_col <- paste0(col, "_sd")
    sd_vals <- ifelse(is.na(agg_results[[sd_col]]), 0, agg_results[[sd_col]])
    formatted_df[[col]] <- sprintf("%.*f (%.*f)", digits, agg_results[[mean_col]], digits, sd_vals)
  }
  
  return(formatted_df)
}

## ───────────────────────────────────────────────────────────────────
## 시뮬레이션 실행
## ───────────────────────────────────────────────────────────────────
source("loss_shifting_v5.R")  # <<< v5를 로드 (precomp 지원 포함)
oracle_xor <- function(Xte) ifelse(Xte[,1] * Xte[,2] >= 0, 1, -1)

N_VEC <- c(200); RATES_POS <- c(0.00, 0.05, 0.10, 0.15, 0.2); RATES_NEG <- c(0.00)
N_TEST <- 800; K_REPS <- 3; SEED <- 25; DIGITS <- 5

cat("\n--- Running Scenario: XOR Data with RBF Kernel (kernlab ksvm + loss_shift precomp) ---\n")

tbl_xor_rbf <- run_grid_fully_parallel(
  n_train       = N_VEC[1], rates_pos     = RATES_POS, rates_neg     = RATES_NEG,
  n_test        = N_TEST, data_gen_fun  = gen_xor_2d, oracle_fun    = oracle_xor,
  kernel        = "radial", k             = K_REPS, seed          = SEED, digits        = DIGITS
)

tbl_xor_rbf$scenario <- "XOR (RBF Kernel, precomp for loss_shift)"
final_results <- tbl_xor_rbf[, c("scenario", setdiff(names(tbl_xor_rbf), "scenario"))]
write.csv(final_results, "xor_precomp_final.csv", row.names = FALSE)
print(final_results)


```

