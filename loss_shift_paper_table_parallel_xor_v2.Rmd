---
title: "loss_shift_paper_table_parallel_xor"
output: html_document
date: "2025-08-17"
---

```{r}
source('discord_alarm.r')
```

```{r}
## ───────────────────────────────────────────────────────────────────
##  Dependencies
## ───────────────────────────────────────────────────────────────────
suppressPackageStartupMessages({
  library(e1071)
  library(glmnet)
  library(knitr); library(kableExtra)
  library(future)
  library(future.apply)
  library(progressr)
})

## ───────────────────────────────────────────────────────────────────
## 1) 표 컬럼 정의 (rate_pos, rate_neg 추가)
##    - e1071_hard / e1071_soft 제거 → e1071 하나로 통일
## ───────────────────────────────────────────────────────────────────
TABLE_COLS <- c(
  "n","p","rate_pos","rate_neg","true",
  "hinge_none","sqhinge_none","logistic_none","exp_none",
  "hinge_soft","sqhinge_soft","logistic_soft","exp_soft",
  "hinge_hard","sqhinge_hard","logistic_hard","exp_hard",
  "e1071","glm","glmnet"
)
DISPLAY_NAMES <- c(
  "n","p","rate_pos","rate_neg","true",
  "hinge-none","sqhinge-none","logistic-none","exp-none",
  "hinge-soft","sqhinge-soft","logistic-soft","exp-soft",
  "hinge-hard","sqhinge-hard","logistic-hard","exp-hard",
  "e1071","glm","glmnet"
)
MODEL_COLS <- setdiff(TABLE_COLS, c("n","p","rate_pos","rate_neg","true"))

acc <- function(y_true, y_pred) mean(y_true == y_pred)

## ───────────────────────────────────────────────────────────────────
## 2) 데이터 생성 함수들
## ───────────────────────────────────────────────────────────────────
# 기존 가우시안 데이터 생성: pos ~ N((1,1), I2), neg ~ N((-1,-1), I2)
gen_gauss_2d <- function(n, p = 2, seed = NULL) {
  stopifnot(p == 2)
  if (!is.null(seed)) set.seed(seed)
  n1 <- n %/% 2; n2 <- n - n1
  Xpos <- cbind(rnorm(n1, mean =  1, sd = 1), rnorm(n1, mean =  1, sd = 1))
  Xneg <- cbind(rnorm(n2, mean = -1, sd = 1), rnorm(n2, mean = -1, sd = 1))
  X <- rbind(Xpos, Xneg)
  y <- c(rep(1, n1), rep(-1, n2))
  list(X = X, y = y)
}

# XOR 데이터 생성
# pos: N((1,1), 0.5*I), N((-1,-1), 0.5*I)
# neg: N((1,-1), 0.5*I), N((-1,1), 0.5*I)
gen_xor_2d <- function(n, p = 2, mu = 1, sd = 0.5, seed = NULL) {
  stopifnot(p == 2)
  if (!is.null(seed)) set.seed(seed)
  
  n_per_quad <- n %/% 4
  n_rem <- n %% 4
  ns <- rep(n_per_quad, 4) + c(rep(1, n_rem), rep(0, 4 - n_rem))

  # Class +1: (mu, mu), (-mu, -mu)
  X_p1 <- cbind(rnorm(ns[1], mean =  mu, sd = sd), rnorm(ns[1], mean =  mu, sd = sd))
  X_p2 <- cbind(rnorm(ns[2], mean = -mu, sd = sd), rnorm(ns[2], mean = -mu, sd = sd))

  # Class -1: (mu, -mu), (-mu, mu)
  X_n1 <- cbind(rnorm(ns[3], mean =  mu, sd = sd), rnorm(ns[3], mean = -mu, sd = sd))
  X_n2 <- cbind(rnorm(ns[4], mean = -mu, sd = sd), rnorm(ns[4], mean =  mu, sd = sd))

  X <- rbind(X_p1, X_p2, X_n1, X_n2)
  y <- c(rep(1, ns[1] + ns[2]), rep(-1, ns[3] + ns[4]))
  
  shuffle_idx <- sample(n)
  list(X = X[shuffle_idx, ], y = y[shuffle_idx])
}

## ───────────────────────────────────────────────────────────────────
## 3) 라벨 플립: 클래스별 비율 (rate_pos, rate_neg)
## ───────────────────────────────────────────────────────────────────
flip_labels_asym <- function(y, rate_pos = 0, rate_neg = 0, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)
  idx_p <- which(y ==  1); kp <- floor(length(idx_p) * rate_pos)
  idx_n <- which(y == -1); kn <- floor(length(idx_n) * rate_neg)
  flip_idx <- c(if (kp > 0) sample(idx_p, kp) else integer(0),
                if (kn > 0) sample(idx_n, kn) else integer(0))
  y2 <- y; if (length(flip_idx)) y2[flip_idx] <- -y2[flip_idx]
  list(y = y2, flipped_idx = flip_idx)
}

## ───────────────────────────────────────────────────────────────────
## 3.5) (신규) GLM/GLMNET용 커널 특징 맵 (RFF for RBF)
## ───────────────────────────────────────────────────────────────────
`%||%` <- function(a, b) if (!is.null(a)) a else b

# 항등 맵(선형)
feature_map_identity <- function(X) X

# RBF 커널의 Random Fourier Features 생성자
# params: list(sigma = 1.0, D = 500, standardize = TRUE, seed = 1)
create_rff_map <- function(params = list()) {
  sigma       <- (params$sigma %||% 1.0)
  D           <- (params$D %||% 500)
  standardize <- (params$standardize %||% TRUE)
  seed        <- (params$seed %||% 1)

  stopifnot(D %% 2 == 0)  # 짝수 권장 (cos/sin 짝)
  gamma <- 1 / (2 * sigma^2)

  set.seed(seed)
  p <- NULL
  # p는 입력 차원. 안전하게 행렬로 변환 후 ncol로 결정
  dummy <- matrix(0, nrow = 1, ncol = 2) # 기본 2D 가정
  p <- ncol(dummy)

  # W를 사전에 만들되, 실제 X의 칼럼 수에 맞게 재생성되도록 클로저 내부에서 점검
  W_init <- NULL
  b_init <- runif(D/2, min = 0, max = 2*pi)

  function(X) {
    X <- as.matrix(X)
    pX <- ncol(X)
    if (is.null(W_init) || nrow(W_init) != pX) {
      # 입력 차원이 달라지면 새로 샘플링
      W <<- matrix(rnorm(pX * (D/2), mean = 0, sd = sqrt(2*gamma)), nrow = pX)
      b <<- b_init
      assign("W_init", W, inherits = TRUE)
      assign("b_init", b, inherits = TRUE)
    } else {
      W <- W_init; b <- b_init
    }
    A  <- X %*% W
    Zc <- cos( sweep(A, 2, b, "+") )
    Zs <- sin( sweep(A, 2, b, "+") )
    Z  <- sqrt(2/(D/2)) * cbind(Zc, Zs)
    if (standardize) Z <- scale(Z, center = TRUE, scale = TRUE)
    colnames(Z) <- NULL
    Z
  }
}

# kernel 문자열과 params로 feature_map을 고르는 팩토리
# kernel = "linear" | "radial"
get_feature_map_for_glm <- function(kernel = "linear", kernel_params = NULL) {
  if (identical(kernel, "radial")) {
    return(create_rff_map(kernel_params %||% list()))
  } else {
    return(feature_map_identity)
  }
}

## ───────────────────────────────────────────────────────────────────
## 4) 외부 모델 래퍼 (e1071, glm, glmnet)
##     - e1071는 하나로 통일 (cost=1.0 소프트마진 기본)
##     - glm/glmnet은 kernel="radial" 시 RFF 사용
## ───────────────────────────────────────────────────────────────────
pred_e1071 <- function(Xtr, ytr, Xte, kernel = "linear", cost = 1.0) {
  y_fac <- factor(ifelse(ytr > 0, "pos", "neg"))
  fit <- e1071::svm(x = Xtr, y = y_fac, kernel = kernel,
                    type = "C-classification", cost = cost, scale = FALSE)
  pr <- predict(fit, Xte)
  ifelse(pr == "pos", 1, -1)
}

pred_glm <- function(Xtr, ytr, Xte, kernel = "linear", kernel_params = NULL) {
  fmap <- get_feature_map_for_glm(kernel, kernel_params)
  Ztr  <- fmap(Xtr)
  Zte  <- fmap(Xte)
  dftr <- data.frame(y = ifelse(ytr > 0, 1, 0), Ztr)
  dfte <- data.frame(Zte)
  fit  <- glm(y ~ ., family = binomial(), data = dftr)
  pr   <- predict(fit, newdata = dfte, type = "response")
  ifelse(pr >= 0.5, 1, -1)
}

pred_glmnet <- function(Xtr, ytr, Xte, alpha = 1,
                        kernel = "linear", kernel_params = NULL,
                        standardize = TRUE, nfolds = 5) {
  fmap <- get_feature_map_for_glm(kernel, kernel_params)
  Ztr  <- fmap(Xtr)
  Zte  <- fmap(Xte)
  y01 <- ifelse(ytr > 0, 1, 0)
  cv  <- glmnet::cv.glmnet(
    x = as.matrix(Ztr), y = y01, family = "binomial",
    alpha = alpha, nfolds = nfolds, type.measure = "deviance",
    standardize = standardize
  )
  pr  <- as.numeric(predict(cv, newx = as.matrix(Zte),
                            s = "lambda.min", type = "response"))
  ifelse(pr >= 0.5, 1, -1)
}

## ───────────────────────────────────────────────────────────────────
## 5) 시뮬레이션 1회
##    - glm/glmnet: kernel, kernel_params 반영 (radial→RFF)
##    - e1071: 단일 모델(e1071)만 계산
## ───────────────────────────────────────────────────────────────────
simulate_once <- function(n_train, n_test, rate_pos, rate_neg,
                          data_gen_fun, oracle_fun, kernel = "linear",
                          kernel_params = NULL,
                          seed = NULL) {
  Tr <- data_gen_fun(n_train, seed = if (is.null(seed)) NULL else seed + 1)
  Te <- data_gen_fun(n_test,  seed = if (is.null(seed)) NULL else seed + 2)
  Xtr <- Tr$X; ytr_clean <- Tr$y
  Xte <- Te$X; yte_true  <- Te$y

  ytr <- flip_labels_asym(ytr_clean, rate_pos = rate_pos, rate_neg = rate_neg,
                          seed = if (is.null(seed)) NULL else seed + 3)$y

  # loss-shifting 12개 (원 코드 유지; 내부에서 kernel="gaussian" 사용)
  preds_ls <- list()
  for (st in c("none","soft","hard")) for (bl in c("hinge","sqhinge","logistic","exp")) {
    set.seed(if (is.null(seed)) NULL else seed + 10*match(st, c("none","soft","hard")) + match(bl, c("hinge","sqhinge","logistic","exp")))
    res <- train_loss_shift(
      X_train = Xtr, y_train = ytr, X_test = Xte,
      base_loss = bl, style = st, kernel = "gaussian",
      restarts = 10, n_folds = 3, svm_dual = TRUE,
      max_iter = 100, line_search = TRUE, verbose = FALSE,
      seed = if (is.null(seed)) NULL else seed + 100
    )
    preds_ls[[paste0(bl,"_",st)]] <- res$test_pred
  }

  # 외부 모델들
  pred_e1071_   <- pred_e1071(Xtr, ytr, Xte, kernel = kernel, cost = 1.0) # 단일 모델
  pred_glm_     <- pred_glm(Xtr, ytr, Xte, kernel = kernel, kernel_params = kernel_params)
  pred_glmnet_  <- pred_glmnet(Xtr, ytr, Xte, alpha = 1,
                               kernel = kernel, kernel_params = kernel_params)

  # 오라클(이론) 경계
  pred_true_oracle <- oracle_fun(Xte)

  c(
    true           = acc(yte_true, pred_true_oracle),
    hinge_none     = acc(yte_true, preds_ls$hinge_none),
    sqhinge_none   = acc(yte_true, preds_ls$sqhinge_none),
    logistic_none  = acc(yte_true, preds_ls$logistic_none),
    exp_none       = acc(yte_true, preds_ls$exp_none),
    hinge_soft     = acc(yte_true, preds_ls$hinge_soft),
    sqhinge_soft   = acc(yte_true, preds_ls$sqhinge_soft),
    logistic_soft  = acc(yte_true, preds_ls$logistic_soft),
    exp_soft       = acc(yte_true, preds_ls$exp_soft),
    hinge_hard     = acc(yte_true, preds_ls$hinge_hard),
    sqhinge_hard   = acc(yte_true, preds_ls$sqhinge_hard),
    logistic_hard  = acc(yte_true, preds_ls$logistic_hard),
    exp_hard       = acc(yte_true, preds_ls$exp_hard),
    e1071          = acc(yte_true, pred_e1071_),
    glm            = acc(yte_true, pred_glm_),
    glmnet         = acc(yte_true, pred_glmnet_)
  )
}

## ───────────────────────────────────────────────────────────────────
## 6) k회 반복 → 평균/표준편차 ("m (sd)" 문자열)
## ───────────────────────────────────────────────────────────────────
mean_sd_fmt <- function(m, s, digits = 3) sprintf(paste0("%.",digits,"f (%.",digits,"f)"), m, s)

simulate_k <- function(n_train, n_test, rate_pos, rate_neg,
                       data_gen_fun, oracle_fun, kernel = "linear",
                       kernel_params = NULL,
                       k = 20, seed = 2025, digits = 3) {
  res_mat <- matrix(NA_real_, nrow = k, ncol = length(c("true", MODEL_COLS)))
  colnames(res_mat) <- c("true", MODEL_COLS)
  for (i in seq_len(k)) {
    A <- simulate_once(
      n_train, n_test, rate_pos, rate_neg,
      data_gen_fun = data_gen_fun,
      oracle_fun   = oracle_fun,
      kernel       = kernel,
      kernel_params = kernel_params,
      seed         = seed + i
    )
    res_mat[i, names(A)] <- A
  }
  means <- colMeans(res_mat, na.rm = TRUE)
  sds   <- apply(res_mat, 2, sd, na.rm = TRUE)
  list(means = means, sds = sds,
       display = setNames(mean_sd_fmt(means, sds, digits), names(means)))
}

## ───────────────────────────────────────────────────────────────────
## 7) 병렬 실행을 위한 그리드 실행 + 표 생성
## ───────────────────────────────────────────────────────────────────
run_grid_parallel <- function(n_vec, rates_pos, rates_neg, n_test,
                              data_gen_fun, oracle_fun, kernel = "linear",
                              kernel_params = NULL,
                              k = 20, seed = 2025, digits = 3) {
  p <- 2
  plan(multisession)
  
  param_grid <- expand.grid(n = n_vec, rp = rates_pos, rn = rates_neg,
                            stringsAsFactors = FALSE)

  cat(sprintf("Running %d simulation sets in parallel (k=%d reps each)...\n", nrow(param_grid), k))

  with_progress({
    p_bar <- progressor(steps = nrow(param_grid))
    
    rows_list <- future_lapply(1:nrow(param_grid), function(i) {
      n_run <- param_grid$n[i]
      rp_run <- param_grid$rp[i]
      rn_run <- param_grid$rn[i]

      R <- simulate_k(
        n_train     = n_run, n_test = n_test,
        rate_pos    = rp_run, rate_neg = rn_run,
        data_gen_fun = data_gen_fun,
        oracle_fun   = oracle_fun,
        kernel       = kernel,
        kernel_params = kernel_params,
        k = k, seed = seed + i, digits = digits
      )
      
      p_bar(sprintf("n=%d, rp=%.2f", n_run, rp_run))
      c(n = n_run, p = p, rate_pos = rp_run, rate_neg = rn_run, R$display)
    }, future.seed = TRUE)
  })

  cat("All simulations complete. Combining results...\n")
  df <- do.call(rbind, rows_list) |> as.data.frame(check.names = FALSE)
  df$n <- as.integer(df$n); df$p <- as.integer(df$p)
  df$rate_pos <- as.numeric(df$rate_pos); df$rate_neg <- as.numeric(df$rate_neg)
  
  df <- df[ , c("n","p","rate_pos","rate_neg","true", MODEL_COLS)]
  df[order(df$n, df$p, df$rate_pos, df$rate_neg), ]
}

```

```{r}
## ───────────────────────────────────────────────────────────────────
## 사용 예시 (Gaussian & XOR)
## ───────────────────────────────────────────────────────────────────

# 1. 학습/예측 함수 로드 (train_loss_shift 함수가 포함된 파일)
source("loss_shifting_v4.R") 

# 2. 데이터셋별 오라클 함수 정의
#oracle_gauss <- function(Xte) ifelse(rowSums(Xte) >= 0, 1, -1)
oracle_xor <- function(Xte) ifelse(Xte[,1] * Xte[,2] >= 0, 1, -1)

# 3. 시뮬레이션 파라미터 설정
N_VEC <- c(200)
RATES_POS <- c(0.00, 0.05, 0.10, 0.15, 0.2)
RATES_NEG <- c(0.00)
N_TEST <- 400
K_REPS <- 30
SEED <- 20250826
DIGITS <- 5

# 4. glm/glmnet에만 적용될 RBF(RFF) 하이퍼파라미터
KPARAMS <- list(sigma = 0.8, D = 600, standardize = TRUE, seed = 7)  # D는 짝수 권장
```


```{r}
cat("\n--- Running Scenario: XOR Data with RBF Kernel (SVM/GLM/GLMNET) ---\n")
tbl_xor_rbf <- run_grid_parallel(
  n_vec         = N_VEC,
  rates_pos     = RATES_POS,
  rates_neg     = RATES_NEG,
  n_test        = N_TEST,
  data_gen_fun  = gen_xor_2d,
  oracle_fun    = oracle_xor,
  kernel        = "radial",      # e1071은 RBF, glm/glmnet도 RFF로 RBF 근사
  kernel_params = KPARAMS,       # 필요
  k             = K_REPS,
  seed          = SEED,
  digits        = DIGITS
)
tbl_xor_rbf$scenario <- "XOR (RBF Kernel)"

# 저장/출력
final_results <- tbl_xor_rbf
final_results <- final_results[, c("scenario", setdiff(names(final_results), "scenario"))]
write.csv(final_results, "0826_results.csv", row.names = FALSE)
print(final_results)


send_alarm(TRUE)

```


